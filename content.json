{"meta":{"title":"Hexo","subtitle":"","description":"","author":"蛋壳","url":"http://example.com","root":"/"},"pages":[{"title":"Tags","date":"2022-05-18T12:05:58.489Z","updated":"2022-05-18T12:05:58.489Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"About","date":"2022-05-18T12:05:58.486Z","updated":"2022-05-18T12:05:58.486Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-05-18T12:05:58.487Z","updated":"2022-05-18T12:05:58.487Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"使用SpringBoot搭建基础的Kafka流处理平台","slug":"eight","date":"2022-06-08T00:49:33.000Z","updated":"2022-06-10T02:10:16.000Z","comments":true,"path":"2022/06/08/eight/","link":"","permalink":"http://example.com/2022/06/08/eight/","excerpt":"","text":"一、调试并启动项目 （1）在(https://github.com/skykip/kafka_rgzn_Training）下载项目 （2）调试项目，设置项目结构 （3）启动项目 二、代码详细注释 （1）ErrorCode （2）MessageEntity （3）Response （4）KafkaConsumerConfig （5）KafkaProducerConfig （6）SimpleConsumer （7）ProducerController （8）ProducerCallback （9）SimpleProducer （10）KafkaDemoApplication （11）application.properties 三、调试KafkaDemoApplication文件 ​ A. 测试时，使用get测试http://localhost:8080/kafka/index接口是否正常。发送相应的json数据。请截图Http接口调试工具中响应体的具体内容和项目运行界面的显示内容。如下图所示： ​ B. 测试时，使用post方式向http://localhost:8080/kafka/send接口发送json数据。 ​ 样例数据如下（请在样例数据中改为自己的姓名和学号）： ​ {“title”:”bigdata19-kafka”,”body”:”2022-06-07，俞佳杭（194800234）的kafka实训”} ​ 请截图Http接口调试工具中响应体的具体内容和项目运行界面的显示内容。如下图所示： ​ C.测试后，项目运行界面的显示内容","categories":[],"tags":[]},{"title":"Eagle部署及各项功能","slug":"seven","date":"2022-06-06T08:02:21.000Z","updated":"2022-06-08T00:47:54.000Z","comments":true,"path":"2022/06/06/seven/","link":"","permalink":"http://example.com/2022/06/06/seven/","excerpt":"","text":"一、Kafka Eagle部署 （1）在&#x2F;export&#x2F;server路径下，上传并解压kafka-eagle-bin-2.1..gz 123cd /export/server/rztar -zxvf kafka-eagle-bin-2.1..gz （2）当解压完kafka-eagle-bin-2.1..gz之后，ls查看，出现kafka-eagle-bin-2.1.0意味着解压成功 （3）进入&#x2F;export&#x2F;server&#x2F;kafka-eagle-bin-2.1.0路径，可发现其中还有一个efak-web-2.1.0.gz解压包，对其解压 12cd /export/server/kafka-eagle-bin-2.1.0tar -zxvf efak-web-2.1.0.gz （4）当解压完efak-web-2.1.0.gz之后，ls查看，出现efak-web-2.1.0意味着解压成功 （5）配置eagle ​ ······修改配置文件system-config.properties 12cd /export/server/kafka-eagle-bin-2.1.0/efak-web-2.1.0/conf/vim system-config.properties ​ ······修改环境变量&#x2F;etc&#x2F;profile，并重新加载 12vim /etc/profilesource /etc/profile （6）启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录 1mkdir /export/data/db （7）启动Eagle（注意：需要开启zookeeper和kafka） 12cd /export/server/kafka-eagle-bin-2.1.0/efak-web-2.1.0/bin/ke.sh start （8）进入本地界面（http://192.168.88.151:8048），出现下图，Eagle部署完成 ​ ······Account:admin ​ ······Password:123456 二、Kafka Eagle各项功能 （1）Dashboard（仪表盘） ​ 查看BROKERS、TOPICS、ZOOKEEPERS、Topic LogSize Top10等 （2）BScreen(大屏) ​ 该模块包含展示消费者和生产者当日及最近7天趋势、Kafka集群读写速度、Kafka集群历史总记录等。 （3）Topics（主题管理） ​ 该模块包含主题创建、主题管理、主题预览、KSQL查询主题、主题数据写入、主题属性配置等。 （4）Consumers（消费监控） ​ 该模块包含监控不同消费者组中的Topic被消费的详情，例如LogSize、Offsets、以及Lag等。同时，支持查看Lag的历史趋势图 （5）Cluster（集群管理） ​ 该模块包含Kafka集群和Zookeeper集群的详情展示，例如Kafka的IP和端口、版本号、启动时间、Zookeeper的Leader和Follower。同时，还支持多Kafka集群切换，以及Zookeeper Client数据查看等功能。 （6）Metrics（集群状态） ​ 该模块包含监控Kafka集群和Zookeeper集群的核心指标，包含Kafka的消息发送趋势、消息大小接收与发送趋势、Zookeeper的连接数趋势等。同时，还支持查看Broker的瞬时指标数据。 （7）Alarm（告警） ​ 该模块包含告警集群异常和消费者应用Lag异常。同时，支持多种IM告警方式，例如邮件、钉钉、微信、Webhook等。 （8）System（系统管理） ​ 该模块包含用户管理，例如创建用户、用户授权、资源管理等","categories":[],"tags":[]},{"title":"kafka API使用方法","slug":"six","date":"2022-06-05T05:38:51.000Z","updated":"2022-06-05T06:03:42.000Z","comments":true,"path":"2022/06/05/six/","link":"","permalink":"http://example.com/2022/06/05/six/","excerpt":"","text":"三、*生产者API* 一个正常的生产逻辑需要具备以下几个步骤 (1)配置生产者客户端参数及创建相应的生产者实例 (2)构建待发送的消息 (3)发送消息 (4)关闭生产者实例 \\1. 新建Maven项目，配置pom.xml \\2. 新建ProducerDemo类，ProducerCallbackDemo类 \\3. 生产者原理 整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender（发送线程）。在主线程中由KafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator,也称为消息收集器）中。Sender线程负责从RecordAccumulator中获取消息并将其发送到Kafka中。 RecordAccumulator 主要用来缓存消息以便Sender线程可以批量发送，进而减少网络传输的资源消耗以提升性能。 主线程中发送过来的消息都会被追加到RecordAccumulator的某个双端队列（Deque）中，在RecordAccumulator的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch,即Deque。消息写入缓存时，追加到双端队列的尾部；Sender读取消息时，从双端队列的头部读取。 消息在网络上都是以字节(Byte)的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在Kafka生产者客户端中，通过java.io.ByteBuffer实现消息内存的创建和释放。不过频繁的创建和释放是比较消耗资源的，在RecordAccumulator的内部还有一个BufferPool，它主要用来实现ByteBuffer的复用，以实现缓存的高效利用。 \\4. ack应答机制 对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功。 所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。 acks &#x3D; 0：生产者只负责发消息，不管Leader 和Follower 是否完成落盘就会发送ack 。这样能够最大降低延迟，但当Leader还未落盘时发生故障就会造成数据丢失。 acks &#x3D; 1：Leader将数据落盘后，不管Follower 是否落盘就会发送ack 。这样可以保证Leader节点内有一份数据，但当Follower还未同步时Leader发生故障就会造成数据丢失。 acks &#x3D; -1(all)：生产者等待Leader 和ISR 集合内的所有Follower 都完成同步才会发送ack 。但当Follower 同步完之后，broker发送ack之前，Leader发生故障时，此时会重新从ISR内选举一个新的Leader，此时由于生产者没收到ack，于是生产者会重新发消息给新的Leader，此时就会造成数据重复。 四、*消费者API* 一个正常的消费逻辑需要具备以下几个步骤: (1)配置消费者客户端参数 (2)创建相应的消费者实例; (3)订阅主题; (4)拉取消息并消费; (5)提交消费位移 offset; (6)关闭消费者实例。 \\1. subscribe 有如下重载方法: public void subscribe(Collection topics,ConsumerRebalanceListener listener) public void subscribe(Collection topics) public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) public void subscribe(Pattern pattern) \\2. 指定集合方式订阅主题 consumer.subscribe(Arrays.asList(topic1)); consumer subscribe(Arrays.asList(topic2)) \\3. 正则方式订阅主题 如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。 \\4. assign 订阅主题 这个方法只接受参数 partitions,用来指定需要订阅的分区集合 \\5. subscribe 与 assign 的区别 (1)通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ; 在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。 (2)assign() 方法订阅分区时,是不具备消费者自动均衡的功能的; 其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。 \\6. 消息的消费模式 Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。 推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息 Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法, poll() 方法返回的是所订阅的主题(分区)上的一组消息。 \\7. 指定位移消费 seek() 方法:从特定的位移处开始拉取消息 \\8. 再均衡监听器 一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救; \\9. 自动位移提交 Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。 (1)重复消费 假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。 (2)丢失消息 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理 \\10. 新建ConsumerDemo，ConsumerDemo1，ConsumerTask，ConsumerDemo2，ConsumerSeekOffset类 五、*Topic管理API* KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法: \\1. 新建KafkAdminDemo，CallableDemo类","categories":[],"tags":[]},{"title":"Kafka命令行操作","slug":"five","date":"2022-06-05T05:38:42.000Z","updated":"2022-06-05T05:56:32.000Z","comments":true,"path":"2022/06/05/five/","link":"","permalink":"http://example.com/2022/06/05/five/","excerpt":"","text":"二、*Kafka命令行操作* Kafka中提供了许多命令行工具(位于$KAFKA HOME&#x2F;bin 目录下)用于管理集群的变更。 kafka-configs.sh 用于配置管理 kafka-console-consumer.sh 用于消费消息 kafka-console-producer.sh 用于生产消息 kafka-console-perf-test.sh 用于测试消费性能 kafka-topics.sh 用于管理主题 kafka-dump-log.sh 用于查看日志内容 kafka-server-stop.sh 用于关闭Kafka服务 kafka-preferred-replica-election.sh 用于优先副本的选举 kafka-server-start.sh 用于启动Kafka服务 kafka-producer-perf-test.sh 用于测试生产性能 kafka-reassign-partitions.sh 用于分区重分配 \\1. 创建topic kafka-topics.sh –create –topic test1 –partitions 1 –replication-factor 2 –zookeeper node1:2181 \\2. 删除topic kafka-topics.sh –delete –topic tpc1 –zookeeper node1:2181 \\3. 查看topic kafka-topics.sh –list –zookeeper node1:2181,node2:2181,node3:2181 _consumer_offsets \\4. 增加分区数 bin&#x2F;kafka-topics.sh –alter –topic tpc_1 –partitions 3 –zookeeper node1:2181 Kafka 只支持增加分区,不支持减少分区 *原因是:减少分区,代价太大(数据的转移,日志段拼接合并)* \\5. 动态配置topic参数 –添加、修改配置参数（开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)） bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –add-config compression.type&#x3D;gzip –删除配置参数 bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –delete-config compression.type \\6. Kafka命令行生产者与消费者操作 –生产者:kafka-console-producer bin&#x2F;kafka-console-producer.sh –broker-list node1:9092, node2:9092, node3:9092 –topic tpc_1 –消费者:kafka-console-consumer bin&#x2F;kafka-console-consumer.sh –bootstrap-server node1:9092, node2:9092, node1:9092 –topic tpc_1 –from-beginning","categories":[],"tags":[]},{"title":"Kafka环境配置","slug":"four","date":"2022-06-05T05:38:29.000Z","updated":"2022-06-05T06:07:30.000Z","comments":true,"path":"2022/06/05/four/","link":"","permalink":"http://example.com/2022/06/05/four/","excerpt":"","text":"一、*Kafka环境配置* 1、上传安装包到&#x2F;export&#x2F;server&#x2F;路径并解压 cd &#x2F;export&#x2F;server rz tar -zxvf kafka_2.11-2.0.0.tgz 2、修改配置文件 （1）进入配置文件目录 cd &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F;config （2）编辑配置文件 –vim server.properties broker.id&#x3D;0 从0开始，依次增加（0,1,2,3,4…），每台不能重复 将Listeners &#x3D; plaintext:&#x2F;&#x2F;:9092改成：Listeners &#x3D; plaintext:&#x2F;&#x2F;node1:9092 #数据存储的目录 log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs #默认分区数 Num.partitions &#x3D; 1 —-Log retention policy—- 数据保留策略 168&#x2F;24&#x3D;7，1073741824&#x2F;1024&#x3D;1GB，300000ms &#x3D; 300s &#x3D; 5min（超过了删掉） #指定 zk 集群地址 zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181 3、分发kafka cd &#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; node2:$PWD scp -r &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; node3:$PWD （注意：分发完后需修改node2，node3中的server.properties文件） node2 node3 4、配置环境变量 vi &#x2F;etc&#x2F;profile（将以下内容添加到profile文件中，三台虚拟机都需要） export KAFKA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;kafka export PATH&#x3D;$PATH:$KAFKA_HOME&#x2F;bin #重新加载环境变量 source &#x2F;etc&#x2F;profile \\1. 启动kafka","categories":[],"tags":[]},{"title":"Spark HA & Yarn配置","slug":"Spark HA & Yarn配置","date":"2022-05-25T04:37:14.436Z","updated":"2022-05-25T04:35:36.680Z","comments":true,"path":"2022/05/25/Spark HA & Yarn配置/","link":"","permalink":"http://example.com/2022/05/25/Spark%20HA%20&%20Yarn%E9%85%8D%E7%BD%AE/","excerpt":"三、Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下 master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。","text":"三、Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下 master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。 此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper 配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接 在 master 节点上重新进行前面配置的 zookeeper 操作 123456789101.上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件2.在 /export/server 目录下创建软连接3.进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去6.将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave27.推送成功后，分别在 node2 和 node3 上创建软连接8.接上步推送完成后将 node2 和 node3 的 /export/server/zookeeper/zkdatas/文件夹下的 myid 中的内容分别改为 2 和 3配置环境变量：因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. 进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容 123cd /export/server/spark/conf vim spark-env.sh 为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master 12345结果显示：...... 82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST=node1......... 文末添加内容 1234SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现# 指定Zookeeper的连接地址# 指定在Zookeeper中注册临时节点的路径 分发 spark-env.sh 到 node2 和 node3 上 123scp spark-env.sh node2:/export/server/spark/conf/scp spark-env.sh node3:/export/server/spark/conf/ 启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群: 12345# 在 node1 上 启动一个master 和全部worker/export/server/spark/sbin/start-all.sh# 注意, 下面命令在 slave1 上执行 启动 node2 上的 master 做备用 master/export/server/spark/sbin/start-master.sh 12345678910111213141516171819结果显示：(base) [root@node1 ~]# jps37328 DataNode41589 Master35798 QuorumPeerMain38521 ResourceManager46281 Jps38907 NodeManager41821 Worker36958 NameNode(base) [root@node2 sbin]# jps36631 DataNode48135 Master35385 QuorumPeerMain37961 NodeManager40970 Worker48282 Jps37276 SecondaryNameNode 访问 WebUI 界面 1http://node1:8081/ ![](Spark HA &amp; Yarn配置&#x2F;1.png) 1http://node2:8082/ ![](Spark HA &amp; Yarn配置&#x2F;2.png)此时 kill 掉 node1 上的 master 假设 master 主机宕机掉 123456789101112# node1主机 master 的进程号kill -9 41589结果显示：(base) [root@node1 ~]# jps37328 DataNode90336 Jps35798 QuorumPeerMain38521 ResourceManager38907 NodeManager41821 Worker36958 NameNode 访问 node2 的 WebUI 1http://node2:8082/ ![](Spark HA &amp; Yarn配置&#x2F;3.png) 进行主备切换的测试 提交一个 spark 任务到当前 活跃的 master上 : 1/export/server/spark/bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 1000 复制标签 kill 掉 master 的 进程号 再次访问 node1 的 WebUI 1http://node1:8081/ 1网页访问不了！ 再次访问 node2 的 WebUI 1http://node2:8082/ ![](Spark HA &amp; Yarn配置&#x2F;4.png) 可以看到当前活跃的 node1 提示信息 123456(base) [root@node1 ~]# /export/server/spark/bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 100022/03/29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect...22/03/29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect...Pi is roughly 3.140960 1同样可以输出结果 当新的 node1 接收集群后, 程序继续运行, 正常得到结果. 结论 HA模式下, 主备切换 不会影响到正在运行的程序. 最大的影响是 会让它中断大约30秒左右. 四、Spark On YARN模式在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端 保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了） 1234567spark-env.sh 文件部分显示：.... 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop.... 链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式） 12345bin/pyspark --master yarn --deploy-mode client|cluster# --deploy-mode 选项是指定部署模式, 默认是 客户端模式# client就是客户端模式# cluster就是集群模式# --deploy-mode 仅可以用在YARN模式下 1bin/spark-shell --master yarn --deploy-mode client|cluster 1bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py 参数 spark-submit 和 spark-shell 和 pyspark的相关参数 1234- bin/pyspark: pyspark解释器spark环境- bin/spark-shell: scala解释器spark环境- bin/spark-submit: 提交jar包或Python文件执行的工具- bin/spark-sql: sparksql客户端工具 12这4个客户端工具的参数基本通用.以spark-submit 为例:bin/spark-submit --master spark://master:7077 xxx.py` 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]Usage: spark-submit --kill [submission ID] --master [spark://...]Usage: spark-submit --status [submission ID] --master [spark://...]Usage: spark-submit run-example [options] example-class [example args]Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://https://host:port, or local (Default: local[*]). --deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client --class CLASS_NAME 运行java或者scala class(for Java / Scala apps). --name NAME 程序的名字 --jars JARS Comma-separated list of jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES 指定Python程序依赖的其它python文件 --files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). --archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor. --conf, -c PROP=VALUE 手动指定配置 --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Driver的可用内存(Default: 1024M). --driver-java-options Driver的一些Java选项 --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Executor的内存 (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h 显示帮助文件 --verbose, -v Print additional debug output. --version, 打印版本 Cluster deploy mode only(集群模式专属): --driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only: --kill SUBMISSION_ID 指定程序ID kill --status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only: --total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only: --executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下): --num-executors NUM Executor应该开启几个 --principal PRINCIPAL Principal to be used to login to KDC. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only: --queue QUEUE_NAME 指定运行的YARN队列(Default: &quot;default&quot;). 启动 YARN 的历史服务器 123cd /export/server/hadoop-3.3.0/sbin./mr-jobhistory-daemon.sh start historyserver 访问WebUI界面 1http://node1:19888/ ![](Spark HA &amp; Yarn配置&#x2F;5.png) client 模式测试 123SPARK_HOME=/export/server/spark $&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3 cluster 模式测试 123SPARK_HOME=/export/server/spark $&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 --conf &quot;spark.pyspark.driver.python=/root/anaconda3/bin/python3&quot; --conf &quot;spark.pyspark.python=/root/anaconda3/bin/python3&quot; $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3","categories":[],"tags":[]},{"title":"Spark local & stand-alone配置","slug":"Spark local& stand-alone配置","date":"2022-05-25T04:37:12.823Z","updated":"2022-05-25T04:35:30.699Z","comments":true,"path":"2022/05/25/Spark local& stand-alone配置/","link":"","permalink":"http://example.com/2022/05/25/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"Spark安装配置Spark是专为大规模数据处理而设计的快速通用的计算引擎，其提供了一个全面、统一的框架用于管理各种不同性质的数据集和数据源的大数据处理的需求，大数据开发需掌握Spark基础、SparkJob、Spark RDD、spark job部署与资源分配、Spark shuffle、Spark内存管理、Spark广播变量、Spark SQL、Spark Streaming以及Spark ML等相关知识。","text":"Spark安装配置Spark是专为大规模数据处理而设计的快速通用的计算引擎，其提供了一个全面、统一的框架用于管理各种不同性质的数据集和数据源的大数据处理的需求，大数据开发需掌握Spark基础、SparkJob、Spark RDD、spark job部署与资源分配、Spark shuffle、Spark内存管理、Spark广播变量、Spark SQL、Spark Streaming以及Spark ML等相关知识。 一、Spark-local模式本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境 Anaconda On Linux 安装 (单台服务器脚本安装) 安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server: 1234cd /export/server# 运行文件sh Anaconda3-2021.05-Linux-x86_64.sh 123456789101112过程显示：...# 出现内容选 yesPlease answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;&gt;&gt;&gt; yes...# 出现添加路径：/export/server/anaconda3...[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3PREFIX=/export/server/anaconda3... 安装完成后, 退出终端， 重新进来: 1exit 1234结果显示：# 看到这个Base开头表明安装好了.base是默认的虚拟环境.Last login: Tue Mar 15 15:28:59 2022 from 192.168.77.1(base) [root@node1 ~]# 创建虚拟环境 pyspark 基于 python3.8 1conda create -n pyspark python=3.8 切换到虚拟环境内 1conda activate pyspark 123结果显示：(base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]# 在虚拟环境内安装包 （有WARNING不用管） 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple spark 安装 将文件上传到 &#x2F;export&#x2F;server 里面 ，解压 1234cd /export/server# 解压tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ 建立软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 添加环境变量 SPARK_HOME: 表示Spark安装路径在哪里PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器JAVA_HOME: 告知Spark Java在哪里HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里HADOOP_HOME: 告知Spark Hadoop安装在哪里 1234567891011121314151617181920212223242526vim /etc/profile内容：注：此部分之前配置过，此部分不需要在配置#JAVA_HOMEexport JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar#HADOOP_HOMEexport HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin#ZOOKEEPER_HOMEexport ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin# 将以下部分添加进去#SPARK_HOMEexport SPARK_HOME=/export/server/spark#HADOOP_CONF_DIRexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop#PYSPARK_PYTHONexport PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 1234567vim .bashrc内容添加进去：#JAVA_HOMEexport JAVA_HOME=/export/server/jdk1.8.0_241 #PYSPARK_PYTHONexport PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 重新加载环境变量文件 12source /etc/profilesource ~/.bashrc 进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹 1cd /export/server/anaconda3/envs/pyspark/bin/ 开启 1./pyspark 1234567891011121314151617181920结果显示：(base) [root@node1 bin]# ./pysparkPython 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Setting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &#x27;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.2.0 /_/Using Python version 3.8.12 (default, Oct 12 2021 13:49:34)Spark context Web UI available at http://master:4040Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1647347826262).SparkSession available as &#x27;spark&#x27;.&gt;&gt;&gt; 查看WebUI界面 123浏览器访问：http://node1:4040/ ![](Spark local&amp; stand-alone配置&#x2F;1.png) 退出 1conda deactivate 二、Spark-Standalone模式Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署) 安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server: 1234cd /export/server# 运行文件sh Anaconda3-2021.05-Linux-x86_64.sh 123456789101112过程显示：...# 出现内容选 yesPlease answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;&gt;&gt;&gt; yes...# 出现添加路径：/export/server/anaconda3...[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3PREFIX=/export/server/anaconda3... 安装完成后, 退出终端， 重新进来: 1exit 1234结果显示：# 看到这个Base开头表明安装好了.base是默认的虚拟环境.Last login: Tue Mar 15 15:28:59 2022 from 192.168.77.1(base) [root@node1 ~]# 在 node1 节点上把 .&#x2F;bashrc 和 profile 分发给 node2 和 node3 1234567#分发 .bashrc :scp ~/.bashrc root@slave1:~/scp ~/.bashrc root@slave2:~/#分发 profile :scp /etc/profile/ root@slave1:/etc/scp /etc/profile/ root@slave2:/etc/ 创建虚拟环境 pyspark 基于 python3.8 1conda create -n pyspark python=3.8 切换到虚拟环境内 1conda activate pyspark 123结果显示：(base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]# 在虚拟环境内安装包 （有WARNING不用管） 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple node1 节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件 1cd /export/server/spark/conf 将文件 workers.template 改名为 workers，并配置文件内容 123456789mv workers.template workersvim workers# localhost删除，内容追加文末：node1node2node3# 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker 将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容 123456789101112131415161718192021222324252627282930313233mv spark-env.sh.template spark-env.shvim spark-env.sh文末追加内容：## 设置JAVA安装目录JAVA_HOME=/export/server/jdk## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoopYARN_CONF_DIR=/export/server/hadoop/etc/hadoop## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上export SPARK_MASTER_HOST=master# 告知sparkmaster的通讯端口export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口SPARK_MASTER_WEBUI_PORT=8080# worker cpu可用核数SPARK_WORKER_CORES=1# worker可用内存SPARK_WORKER_MEMORY=1g# worker的工作通讯地址SPARK_WORKER_PORT=7078# worker的 webui地址SPARK_WORKER_WEBUI_PORT=8081## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot; 开启 hadoop 的 hdfs 和 yarn 集群 123start-dfs.shstart-yarn.sh 在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下: 123hadoop fs -mkdir /sparkloghadoop fs -chmod 777 /sparklog 将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置 1234567891011mv spark-defaults.conf.template spark-defaults.confvim spark-defaults.conf文末追加内容为：# 开启spark的日期记录功能spark.eventLog.enabled true# 设置spark日志记录的路径spark.eventLog.dir hdfs://node1:8020/sparklog/ # 设置spark日志是否启动压缩spark.eventLog.compress true 配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为 log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息） 123mv log4j.properties.template log4j.propertiesvim log4j.properties 12345结果显示：...18 # Set everything to be logged to the console19 log4j.rootCategory=WARN, console.... node1 节点分发 spark 安装文件夹 到 node2 和 node3 上 12345cd /export/server/scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWDscp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD 在slave1 和 slave2 上做软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 重新加载环境变量 1source /etc/profile 进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh 123cd /export/server/spark/sbin ./start-history-server.sh 访问 WebUI 界面 123浏览器访问：http://node1:18080/ ![](Spark local&amp; stand-alone配置&#x2F;2.png) 启动Spark的Master和Worker进程 1234567891011121314151617# 启动全部master和workersbin/start-all.sh# 或者可以一个个启动:# 启动当前机器的mastersbin/start-master.sh# 启动当前机器的workersbin/start-worker.sh# 停止全部sbin/stop-all.sh# 停止当前机器的mastersbin/stop-master.sh# 停止当前机器的workersbin/stop-worker.sh 访问 WebUI界面 123浏览器访问：http://node1:8080/(可能会发生顺延至8081) ![](Spark local&amp; stand-alone配置&#x2F;3.png)","categories":[],"tags":[]},{"title":"Spark基础环境配置","slug":"Spark基础配置","date":"2022-05-25T04:37:10.997Z","updated":"2022-05-25T04:35:21.739Z","comments":true,"path":"2022/05/25/Spark基础配置/","link":"","permalink":"http://example.com/2022/05/25/Spark%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/","excerpt":"一、安装配置 jdk 编译环境软件安装目录 1mkdir -pv /export/server JDK 1.8安装 rz上传并解压 jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 1tar -zxvf jdk-8u241-linux-x64.tar.gz -C /export/server","text":"一、安装配置 jdk 编译环境软件安装目录 1mkdir -pv /export/server JDK 1.8安装 rz上传并解压 jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 1tar -zxvf jdk-8u241-linux-x64.tar.gz -C /export/server 配置环境变量 12345vim /etc/profileexport JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 重新加载环境变量文件 1source /etc/profile 查看 java 版本号 1java -version 12345结果显示：[root@node1 jdk1.8.0_241]# java -versionjava version &quot;1.8.0_241&quot;Java(TM) SE Runtime Environment (build 1.8.0_241-b07)Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) master 节点将 java 传输到 slave1 和 slave2 12scp -r /export/server/jdk1.8.0_241/ root@node2:/export/server/scp -r /export/server/jdk1.8.0_241/ root@node3:/export/server/ 配置 node2 和 node3 的 jdk 环境变量（注：和上方 node1 的配置方法一样） 在 node1 node2 和node3 创建软连接 123cd /export/serverln -s jdk1.8.0_241/ jdk 重新加载环境变量文件 1source /etc/profile 二、zookeeper安装配置 配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 node1.root node2.root node3.root 123456789vim /etc/hosts#结果显示127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.77.151 node1 node1.root192.168.77.152 node2 node2.root192.168.77.153 node3 node3.root zookeeper安装 rz上传zookeeper-3.4.10.tar.gz并解压到&#x2F;export&#x2F;server&#x2F;目录下 1tar -zxvf zookeeper-3.4.10.tar.gz -C /export/server 在 &#x2F;export&#x2F;server 目录下创建软连接 123cd /export/serverln -s zookeeper-3.4.10/ zookeeper 进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 123cd /export/server/zookeeper/conf/ cp zoo_sample.cfg zoo.cfg 接上步给 zoo.cfg 添加内容 1234567891011121314#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888 进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去 12345cd /export/server/zookeeper/zkdatatouch myidecho &#x27;1&#x27; &gt; myid 将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给node2 和 node3 123scp -r /export/server/zookeeper-3.4.10/ node2:$PWDscp -r /export/server/zookeeper-3.4.10/ node3:$PWD 推送成功后，分别在 node2 和 node3 上创建软连接 1ln -s zookeeper-3.4.10/ zookeeper 接上步推送完成后将 node2 和 node3 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid 中的内容分别改为 2 和 3 123456789cd /export/server/zookeeper/zkdatas/结果显示：[root@node2 zkdatas]# vim myid [root@node2 zkdatas]# more myid 2[root@node3 zkdatas]# vim myid [root@node3 zkdatas]# more myid 3 配置zookeeper的环境变量（注：三台主机都需要配置） 12345vim /etc/profile# zookeeper 环境变量export ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin 重新加载环境变量文件 1source /etc/profile 进入 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做） 123cd /export/server/zookeeper-3.4.10/bin zkServer.sh start 12345结果显示：[root@node1 bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 查看 zookeeper 的状态 1zkServer.sh status 123456789101112131415结果显示：[root@node1 server]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[root@node2 server]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader[root@node3 conf]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower 1jps 123456789101112结果显示：[root@node1 server]# jps125348 QuorumPeerMain16311 Jps[root@node2 server]# jps126688 QuorumPeerMain17685 Jps[root@node3 conf]# jps126733 QuorumPeerMain17727 Jps 脚本一键启动 1234567891011121314151617181920212223242526272829vim zkAll.sh#!/bin/bashif [ $# -eq 0 ];then echo &quot;please input param:start stop&quot;else if [ $1 = start ];then for i in &#123;1..3&#125; do echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot; ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot; done fi if [ $1 = stop ];then for i in &#123;1..3&#125; do echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot; ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot; done fi if [ $1 = status ];then for i in &#123;1..3&#125; do echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot; ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot; done fifi 12# 将文件放在 /bin 目录下chmod +x zkAll.sh &amp;&amp; zkAll.sh 三、Hadoop 安装配置 把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 &#x2F;export&#x2F;server 并解压文件 1tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 修改配置文件(进入路径 &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop) 1cd /export/server/hadoop-3.3.0/etc/hadoop hadoop-env.sh 12345678#文件最后添加export JAVA_HOME=/export/server/jdk1.8.0_241export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root core-site.xml 12345678910111213141516171819202122232425262728293031323334&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; hdfs-site.xml 12345&lt;!-- 设置SNN进程运行机器位置信息 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt; mapred-site.xml 1234567891011121314151617181920212223242526272829303132&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt; yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; workers 123node1node2node3 分发同步hadoop安装包 1234cd /export/serverscp -r hadoop-3.3.0 root@node2:$PWDscp -r hadoop-3.3.0 root@node3:$PWD 将hadoop添加到环境变量 1234vim /etc/profileexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 重新加载环境变量文件 1source /etc/profile Hadoop集群启动 格式化namenode（只有首次启动需要格式化） 1hdfs namenode -format 脚本一键启动 12345678910111213[root@node1 ~]# start-dfs.sh Starting namenodes on [master]上一次登录：五 3月 11 21:27:24 CST 2022pts/0 上Starting datanodes上一次登录：五 3月 11 21:27:32 CST 2022pts/0 上Starting secondary namenodes [slave1]上一次登录：五 3月 11 21:27:35 CST 2022pts/0 上[root@node1 ~]# start-yarn.sh Starting resourcemanager上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上Starting nodemanagers上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上 启动后 输入 jps 查看 1234567891011121314151617[root@node1 ~]# jps127729 NameNode127937 DataNode14105 Jps128812 NodeManager128591 ResourceManager[root@node1 hadoop]# jps121889 NodeManager121559 SecondaryNameNode7014 Jps121369 DataNode[root@node1 hadoop]# jps6673 Jps121543 NodeManager121098 DataNode WEB页面 HDFS集群： 1http://node1:9870/ YARN集群： 1http://node1:8088/ ​","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-05-18T07:06:13.868Z","updated":"2022-05-25T04:48:45.650Z","comments":true,"path":"2022/05/18/hello-world/","link":"","permalink":"http://example.com/2022/05/18/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deploymenttitle: Spark HA &amp; Yarn配置三、Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下 master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deploymenttitle: Spark HA &amp; Yarn配置三、Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下 master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。 此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper 配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接 在 master 节点上重新进行前面配置的 zookeeper 操作 123456789101.上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件2.在 /export/server 目录下创建软连接3.进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去6.将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave27.推送成功后，分别在 node2 和 node3 上创建软连接8.接上步推送完成后将 node2 和 node3 的 /export/server/zookeeper/zkdatas/文件夹下的 myid 中的内容分别改为 2 和 3配置环境变量：因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. 进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容 123cd /export/server/spark/conf vim spark-env.sh 为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master 12345结果显示：...... 82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST=node1......... 文末添加内容 1234SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现# 指定Zookeeper的连接地址# 指定在Zookeeper中注册临时节点的路径 分发 spark-env.sh 到 node2 和 node3 上 123scp spark-env.sh node2:/export/server/spark/conf/scp spark-env.sh node3:/export/server/spark/conf/ 启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群: 12345# 在 node1 上 启动一个master 和全部worker/export/server/spark/sbin/start-all.sh# 注意, 下面命令在 slave1 上执行 启动 node2 上的 master 做备用 master/export/server/spark/sbin/start-master.sh 12345678910111213141516171819结果显示：(base) [root@node1 ~]# jps37328 DataNode41589 Master35798 QuorumPeerMain38521 ResourceManager46281 Jps38907 NodeManager41821 Worker36958 NameNode(base) [root@node2 sbin]# jps36631 DataNode48135 Master35385 QuorumPeerMain37961 NodeManager40970 Worker48282 Jps37276 SecondaryNameNode 访问 WebUI 界面 1http://node1:8081/ ![](Spark HA &amp; Yarn配置&#x2F;1.png) 1http://node2:8082/ ![](Spark HA &amp; Yarn配置&#x2F;2.png)此时 kill 掉 node1 上的 master 假设 master 主机宕机掉 123456789101112# node1主机 master 的进程号kill -9 41589结果显示：(base) [root@node1 ~]# jps37328 DataNode90336 Jps35798 QuorumPeerMain38521 ResourceManager38907 NodeManager41821 Worker36958 NameNode 访问 node2 的 WebUI 1http://node2:8082/ ![](Spark HA &amp; Yarn配置&#x2F;3.png) 进行主备切换的测试 提交一个 spark 任务到当前 活跃的 master上 : 1/export/server/spark/bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 1000 复制标签 kill 掉 master 的 进程号 再次访问 node1 的 WebUI 1http://node1:8081/ 1网页访问不了！ 再次访问 node2 的 WebUI 1http://node2:8082/ ![](Spark HA &amp; Yarn配置&#x2F;4.png) 可以看到当前活跃的 node1 提示信息 123456(base) [root@node1 ~]# /export/server/spark/bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 100022/03/29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect...22/03/29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect...Pi is roughly 3.140960 1同样可以输出结果 当新的 node1 接收集群后, 程序继续运行, 正常得到结果. 结论 HA模式下, 主备切换 不会影响到正在运行的程序. 最大的影响是 会让它中断大约30秒左右. 四、Spark On YARN模式在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端 保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了） 1234567spark-env.sh 文件部分显示：.... 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop.... 链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式） 12345bin/pyspark --master yarn --deploy-mode client|cluster# --deploy-mode 选项是指定部署模式, 默认是 客户端模式# client就是客户端模式# cluster就是集群模式# --deploy-mode 仅可以用在YARN模式下 1bin/spark-shell --master yarn --deploy-mode client|cluster 1bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py 参数 spark-submit 和 spark-shell 和 pyspark的相关参数 1234- bin/pyspark: pyspark解释器spark环境- bin/spark-shell: scala解释器spark环境- bin/spark-submit: 提交jar包或Python文件执行的工具- bin/spark-sql: sparksql客户端工具 12这4个客户端工具的参数基本通用.以spark-submit 为例:bin/spark-submit --master spark://master:7077 xxx.py` 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]Usage: spark-submit --kill [submission ID] --master [spark://...]Usage: spark-submit --status [submission ID] --master [spark://...]Usage: spark-submit run-example [options] example-class [example args]Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://https://host:port, or local (Default: local[*]). --deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client --class CLASS_NAME 运行java或者scala class(for Java / Scala apps). --name NAME 程序的名字 --jars JARS Comma-separated list of jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES 指定Python程序依赖的其它python文件 --files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). --archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor. --conf, -c PROP=VALUE 手动指定配置 --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Driver的可用内存(Default: 1024M). --driver-java-options Driver的一些Java选项 --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Executor的内存 (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h 显示帮助文件 --verbose, -v Print additional debug output. --version, 打印版本 Cluster deploy mode only(集群模式专属): --driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only: --kill SUBMISSION_ID 指定程序ID kill --status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only: --total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only: --executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下): --num-executors NUM Executor应该开启几个 --principal PRINCIPAL Principal to be used to login to KDC. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only: --queue QUEUE_NAME 指定运行的YARN队列(Default: &quot;default&quot;). 启动 YARN 的历史服务器 123cd /export/server/hadoop-3.3.0/sbin./mr-jobhistory-daemon.sh start historyserver 访问WebUI界面 1http://node1:19888/ ![](Spark HA &amp; Yarn配置&#x2F;5.png) client 模式测试 123SPARK_HOME=/export/server/spark $&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3 cluster 模式测试 123SPARK_HOME=/export/server/spark $&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 --conf &quot;spark.pyspark.driver.python=/root/anaconda3/bin/python3&quot; --conf &quot;spark.pyspark.python=/root/anaconda3/bin/python3&quot; $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3","categories":[],"tags":[]},{"title":"Spark HA & Yarn配置","slug":"three","date":"2022-05-17T05:42:06.000Z","updated":"2022-05-17T06:44:30.000Z","comments":true,"path":"2022/05/17/three/","link":"","permalink":"http://example.com/2022/05/17/three/","excerpt":"","text":"*七、Spark-Standalone-HA模式* 注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台主机的旧版zookeeper以及对应的软连接。 在node1节点上重新进行前面配置的zookerper操作 \\1. 上传apache-zookeeper-3.7.0-bin.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 123cd /export/server/tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz \\2. 在&#x2F;export&#x2F;server&#x2F;目录下创建软连接 123cd /export/server/ln -s apache-zookeeper-3.7.0-bin spark \\3. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;将zoo_sample.cfg文件复制为新文件 zoo.cfg \\4. 接上步给zoo.cfg 添加内容 \\5. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将1写入进去 \\6. 将node1节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容分发给node2和node3 \\7. 分发完后，分别在node2和node3上创建软连接 \\8. 将node2和node3的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的myid中的内容分别改为2和3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. 1cd /export/server/spark/conf 1vim spark-env.sh 删除: SPARK_MASTER_HOST&#x3D;node1 在文末添加内容 1234567891011SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER - Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha&quot; \\# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 \\# 指定Zookeeper的连接地址 \\# 指定在Zookeeper中注册临时节点的路径 \\9. 分发spark-env.sh到node2和node3上 123scp spark-env.sh node2:/export/server/spark/conf/ scp spark-env.sh node3:/export/server/spark/conf/ \\10. 启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群: # 在node1上 启动一个master 和全部worker 1sbin/start-all.sh # 注意, 下面命令在node2上执行 1sbin/start-master.sh # 在node2上启动一个备用的master进程 #将node1的master kill掉，查看node2的WebUI界面 *八、Spark-yarn模式* 1、启动yarn的历史服务器，jps看进程 2、在yarn上启动pyspark 3、命令测试 4、提交任务测试 5、client模式测试pi 6、cluster模式测试pi","categories":[],"tags":[]},{"title":"Spark local& stand-alone配置","slug":"two","date":"2022-05-17T04:58:19.000Z","updated":"2022-05-17T05:23:42.000Z","comments":true,"path":"2022/05/17/two/","link":"","permalink":"http://example.com/2022/05/17/two/","excerpt":"","text":"*五、Spark-local模式* \\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件 123cd /export/server/sh Anaconda3-2021.05-Linux-x86_64.sh \\2. 过程显示： 12345678910111213...# 出现内容选 yes Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27; &gt;&gt;&gt; yes... # 出现添加路径：/export/server/anaconda3...[/root/anaconda3]&gt;&gt;&gt;/export/server/anaconda3 PREFIX=/export/server/anaconda3... \\3. 安装完成后，重新启动 看到base就表示安装完成了 \\4. 创建虚拟环境pyspark基于python3.8 1conda create -n pyspark python=3.8 \\5. 切换到虚拟环境内 1conda activate pyspark \\6. 在虚拟环境内安装包 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple \\7. 上传并解压spark-3.2.0-bin-hadoop3.2.tgz 123cd /export/servertar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ \\8. 创建软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark \\9. 添加环境变量 1vim /etc/profile SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 1vim .bashrc 内容添加进去： 1234567#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python \\10. 重新加载环境变量 123source /etc/profilesource ~/.bashrc \\11. 开启spark 123cd /export/server/anaconda3/ens/pyspark/bin/./pyspark \\12. 进入WEB界面（node1:4040&#x2F;） \\13. 退出 1conda deactivate *六、Spark-Standalone模式* \\1. 在node2、node3上安装Python(Anaconda) 出现base表明安装完成 \\2. 将node1上的profile和.&#x2F;bashrc分发给node2、node3 #分发.bashrc 123scp ~/.bashrc root@node2:~/scp ~/.bashrc root@node3:~/ #分发profile 123scp /etc/profile/ root@node2:/etc/scp /etc/profile/ root@node3:/etc/ \\3. 创建虚拟环境pyspark基于python3.8 1conda create -n pyspark python=3.8 \\4. 切换到虚拟环境 1conda activate pyspark \\5. 在虚拟环境内安装包 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple \\6. 修改配置文件 1cd /export/server/spark/conf -配置workers 1mv workers.template workers 1vim workers # 将里面的localhost删除, 追加 12345node1 node2 node3 -配置spark-env.sh 1mv spark-env.sh.template spark-env.sh 1vim spark-env.sh 在底部追加如下内容 12345678910111213141516171819## 设置JAVA安装目录 JAVA_HOME=/export/server/jdk ## HADOOP软件配置文件目录,读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080 # worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078# worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081 ## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=&quot;Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ Dspark.history.fs.cleaner.enabled=true&quot; \\7. 在HDFS上创建程序运行历史记录存放的文件夹: 123hadoop fs -mkdir /sparklog hadoop fs -chmod 777 /sparklog -配置spark-defaults.conf.template 1mv spark-defaults.conf.template spark-defaults.conf 1vim spark-defaults.conf # 修改内容, 追加如下内容 12345# 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://node1:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true -配置log4j.properties 1mv log4j.properties.template log4j.properties 1vim log4j.properties \\8. 将node1的spark分发到node2、node3 12345cd /export/server/scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWDscp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD \\9. 在node2和node3上做软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark \\10. 重新加载环境变量 1source /etc/profile \\11. 启动历史服务器 123cd /export/server/spark/sbin./start-history-server.sh \\12. 访问WebUI界面（http://node1:18080/） \\13. 启动Spark的Master和Worker # 启动全部master和worker sbin&#x2F;start-all.sh # 或者可以一个个启动: # 启动当前机器的master 1sbin/start-master.sh # 启动当前机器的worker 1sbin/start-worker.sh # 停止全部 1sbin/stop-all.sh # 停止当前机器的master 1sbin/stop-master.sh # 停止当前机器的worker 1sbin/stop-worker.sh \\14. 访问WebUI界面（http://node1:8080/）","categories":[],"tags":[]},{"title":"Spark基础配置","slug":"one","date":"2022-05-17T04:42:31.000Z","updated":"2022-05-17T04:57:16.000Z","comments":true,"path":"2022/05/17/one/","link":"","permalink":"http://example.com/2022/05/17/one/","excerpt":"","text":"*一、配置基础环境* #主机名 1cat /etc/hostname # hosts映射 1vim /etc/hosts 1234567891011127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.151 node1.itcast.cn node1 192.168.88.152 node2.itcast.cn node2 192.168.88.153 node3.itcast.cn node3 *二、安装配置jdk* \\1. 编译环境软件安装目录 1mkdir -p /export/server \\2. 上传jdk-8u65-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录并解压 123rztar -zxvf jdk-8u65-linux-x64.tar.gz \\3. 配置环境变量 1vim /etc/profile 12345export JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar \\4. 重新加载环境变量文件 1source /etc/profile \\5. 查看java版本号 1Java -version \\6. 将java由node1分发到node2、node3 123scp -r /export/server/jdk1.8.0_241/ root@node2:/export/serverscp -r /export/server/jdk1.8.0_241/ root@node3:/export/server \\7. 配置node2、node3的环境变量文件（方法如上） \\8. 在node1、node2、node3中创建软连接（三台都需要操作） 123cd /export/server/ln -s jdk1.8.0_241/ jdk *三、Hadoop安装配置* \\1. 上传hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 &#x2F;export&#x2F;server 并解压文件 1tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz \\2. 修改配置文件 1cd /export/server/hadoop-3.3.0/etc/hadoop - hadoop-env.sh #文件最后添加 12345678910111213export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root - core-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;!-- 整合hive 用户代理设置 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!-- 文件系统垃圾桶保存时间 --&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; - hdfs-site.xml 123456789 &lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt; - mapred-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 &lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- MR程序历史服务地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt; - yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 &lt;!-- 设置YARN集群主角色运行机器位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否将对容器实施物理内存限制 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启日志聚集 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置yarn历史服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史日志保存的时间 7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; - workers 12345node1.itcast.cnnode2.itcast.cnnode3.itcast.cn \\3. 将node1的hadoop-3.3.0分发到node2、node3 12345cd /export/serverscp -r /export/server/hadoop-3.3.0 root@node2:$PWDscp -r /export/server/hadoop-3.3.0 root@node3:$PWD \\4. 将hadoop添加到环境变量 1vim /etc/profile 123export HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin \\5. 重新加载环境变量文件 1source /etc/profile \\6. Hadoop集群启动 （1）格式化namenode（只有首次启动需要格式化） 1hdfs namenode -format （2）脚本一键启动 \\7. WEB界面 （1）HDFS集群：http://node1:9870/ （2）YARN集群：http://node1:8088/ *四、zookeeper安装配置* \\1. 上传zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 123cd /export/server/tar -zxvf zookeeper-3.4.10.tar.gz \\2. 创建软连接 123cd /export/server/ln -s zookeeper-3.4.10/ zookeeper \\3. 修改配置文件 1cd /export/server/zookeeper/conf/ (1)将zoo_sample.cfg复制为zoo.cfg 1cp zoo_sample.cfg zoo.cfg 1vim zoo.cfg 将zoo.cfg修改为以下内容 12345678910111213141516171819#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas\\# 保留多少个快照autopurge.snapRetainCount=3\\# 日志多少小时清理一次autopurge.purgeInterval=1\\# 集群中服务器地址server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 (2) 在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1 1echo 1 &gt; /export/server/zookeeper/zkdatas/myid (3) 将node1中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10分发给node2、node3 123scp -r /export/server/zookeeper-3.4.10/ slave1:$PWD scp -r /export/server/zookeeper-3.4.10/ slave2:$PWD (4) 在node2和node3上创建软连接 1ln -s zookeeper-3.4.10/ zookeeper (5) 分别在node2、node3上修改myid的值为2，3 12345cd /export/server/echo 2 &gt; /export/server/zookeeper/zkdatas/myidecho 3 &gt; /export/server/zookeeper/zkdatas/myid (6) 配置zookeeper的环境变量（三台都需要配置） 12345vim /etc/profileexport ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin (7) 重新加载环境变量 1source /etc/profile (8) 三台机器开启zookeeper 123cd /export/server/zookerper-3.4.10/binzkServer.sh start (9) 结果显示 (10) 查看zookeeper状态","categories":[],"tags":[]}],"categories":[],"tags":[]}